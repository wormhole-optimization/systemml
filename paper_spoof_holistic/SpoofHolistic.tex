\documentclass{vldb}

\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11.0in}

\usepackage{amsmath}
\usepackage{amssymb}

%\usepackage[algo2e,ruled,vlined]{algorithm2e}
\usepackage{graphicx}
\usepackage{color}
\usepackage{euscript}
\usepackage{subfigure}
\usepackage[np,autolanguage]{numprint}
\usepackage{units}
\usepackage{balance}
\usepackage{algorithm}
\usepackage[noend]{algorithmic}

\usepackage[pdftex,pdfpagelabels=false]{hyperref} %weiter: ,plainpages=false
\hypersetup{
    unicode=false,          % non-Latin characters in Acrobat’s bookmarks
    pdftoolbar=true,        % show Acrobat’s toolbar?
    pdfmenubar=true,        % show Acrobat’s menu?
    pdffitwindow=true,      % window fit to page when opened
    pdfstartview={FitV},    % fits the width of the page to the window
    pdftitle={Holistic Sum-Product Optimization for Large-Scale Machine Learning},
    pdfauthor={Dylan Hutchison, Matthias Boehm, Alexandre V. Evfimievski, Berthold Reinwald, Prithviraj Sen},
    pdfsubject={},   % subject of the document
    pdfcreator={},   % creator of the document
    pdfproducer={}, % producer of the document
    pdfkeywords={}, % list of keywords
    pdfnewwindow=true,      % links in new window
    bookmarksnumbered=true, % put section numbers in bookmarks
    bookmarksopen=true,     % open up bookmark tree
    bookmarksopenlevel=2,   % \maxdimen level to which bookmarks are open
    colorlinks=false,        % false: boxed links; true: colored links
    linkcolor=red,        % color of internal links  
    citecolor=green,         % color of links to bibliography
    filecolor=black,        % color of file links
    urlcolor=black          % color of external links
}


% math commands
\newcommand{\mat}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\card}[1]{\lvert #1\rvert}
\newcommand{\num}[1]{\numprint{#1}}

% unit commands
\newcommand{\h}{\unit{\,h}}
\newcommand{\s}{\unit{\,s}}
\newcommand{\ms}{\unit{\,ms}}
\newcommand{\bb}{\unit{\,B}}
\newcommand{\mb}{\unit{\,MB}}
\newcommand{\gb}{\unit{\,GB}}
\newcommand{\tb}{\unit{\,TB}}
\newcommand{\mbs}{\unit{\,MB/s}} %{~\unitfrac{MB}{s}}
\newcommand{\gbs}{\unit{\,GB/s}} %{~\unitfrac{GB}{s}} 
\newcommand{\gflops}{\unit{\,GFLOP/s}} %{~\unitfrac{GFLOP}{s}}

% algorithm commands
\renewcommand{\algorithmiccomment}[1]{\hfill \textit{// #1}}
\newcommand{\COMMENTLINE}[1]{\STATE \textit{// #1}}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\renewcommand{\algorithmicrepeat}{\textbf{do}}
\renewcommand{\algorithmicuntil}{\textbf{while}}

%\SetKwRepeat{Do}{do}{while}
%\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}

% format specific commands
\newenvironment{itemize*}{\begin{itemize}\setlength{\itemsep}{2.5pt}\setlength{\parskip}{0pt}\setlength{\parsep}{0pt}}{\end{itemize}}
\renewcommand{\footnotesize}{\small}
\newcommand{\eat}[1]{}
\setlength{\textfloatsep}{9pt}	

\DeclareRobustCommand{\ojoin}{\rule[0.10ex]{.3em}{.4pt}\llap{\rule[1.40ex]{.3em}{.4pt}}}
\newcommand{\leftouterjoin}{\mathrel{\ojoin\mkern-6.5mu\Join}}
\newcommand{\rightouterjoin}{\mathrel{\Join\mkern-6.5mu\ojoin}}
\newcommand{\fullouterjoin}{\mathrel{\ojoin\mkern-6.5mu\Join\mkern-6.5mu\ojoin}}
\newtheorem{definition}{Definition}
\DeclareMathOperator*{\argmin}{arg\,min}

\clubpenalty = 10000
\widowpenalty = 10000
\sloppy
\frenchspacing

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Holistic Sum-Product Optimization\\for Large-Scale Machine Learning}

\numberofauthors{1}
\author{
  \alignauthor 
	~\vspace{-0.7cm}\\
	Dylan Hutchison\textsuperscript{2}\thanks{Work done during an internship at IBM Research -- Almaden.},~~~Matthias Boehm\textsuperscript{1},~~~Alexandre V. Evfimievski\textsuperscript{1},\vspace{0.1cm}\\
  Berthold Reinwald\textsuperscript{1},~~~Prithviraj Sen\textsuperscript{1}\\~\\
  \affaddr{\textsuperscript{1} IBM Research -- Almaden;~~San Jose, CA, USA}\\
	\affaddr{\textsuperscript{2} University of Washington;~~Seattle, WA, USA}
}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
blah blah
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

\begin{enumerate}
\item ML is often expressed in terms of LA.
\item Rewriting is important: LA is rich with identities that, when computed literally, drastically alter the computation runtime of training and inference. 
\item Rewriting is a hard problem: the space of equivalent expressions of a ML script is large.  No deterministic set of rules can find the optimal (fastest) expressions without overwhelming engineering and maintenance efforts.
\item Related work has addressed the problem of rewriting in limited ways: sum-product optimization frameworks (FAQ \cite{KhamisNR16}, AJAR, PANDA) provide a method to find the optimal expression for a limited class of expressions: sums over the product of distinct inputs, wherein the sum operations are semiring (or identical) to a unique product operation.
\item Real world ML scripts have more complexity: CSEs, selections, unary functions, multiple product types, and operator fusion within distributed execution.
\item Our work aims to expand the scope of sum-product optimization to tackle these additional issues present in real-world ML scripts. We present an addition to SystemML called SPOOF---sum-product optimization and operator fusion---that addresses these challenges and accelerates real-world ML scripts by XXX times.
\item SystemML already has an optimization framework that consists of a first phase of deterministic rewriting and a second phase of fusing operators together. While SystemML does achieve speedup with this simplified framework, it leaves plenty of speedup on the table on account of the following factors:
  \begin{enumerate}
  \item Most of SystemML's optimizations operate locally on a small sub-expression of operators. Optimizing at the local level may produce locally optimal yet globally suboptimal expressions. In SPOOF, we aim to widen the scope of optimization to entire DAGs at a time, avoiding expressions that are local minima in terms of runtime.
  \item SystemML uses a deterministic list of rules (ordered and guarded by conditions) for most optimizations. This list of rules may miss optimization opportunities and leads to a long-term maintenance burden: every time it is necessary to add a new rule, the rule list must be carefully engineered so as not to produce suboptimal rule interactions.

  The fundamental issue is that deterministic rule lists do not establish a \emph{search space completeness}.
  We call a rewriting algorithm complete if for every query (initial expression) and input data, the optimal expression equivalent to the query for that data can be found using the rewriting algorithm.
  Thus, the rule list must be updated every time a user presents a query that the rule list does not optimally rewrite. This issue is further compounded by the fact that SystemML uses compound operators such as \texttt{trace(X)}, which overlaps with rewrites that concern matrix sub-selection (of the diagonal, in this case) and summations.

  SPOOF aims to achieve search space completeness by using a cost-based optimization method on an algera of elementary operators. These elementary operators include sums and products over scalar, and the rewrites we consider over them involve basic algebraic rewrites such as the commutative and distributive laws. We also expect a much lower long-term maintenance buden on account of using such a minimal set of operators and rewrites.
  \item SystemML rewrites expressions separately from fusing expressions together. While the separation between rewrite and fusion simplifies SystemML's architecture, it can lead to suboptimal plans: a expression that is more expensive to compute without fusion may be the optimal expression with fusion.
  SPOOF aims to holistically consider fusion and rewrites at the same time.
  \end{enumerate}
\item Use sum($(X - UV^T)^2$) as a motivating example.

\end{enumerate}

\section{System Architecture}
\label{sec:sysarch}

\subsection{SystemML}

\subsection{SPlan Representation}
\begin{enumerate}
\item Named attributes --- concept of ``plates'' or ``boundaries of for loops over attribtues, iterating over scalars''
\item Generator nodes (Read, DataGen, Ext) --- 0 or 1-ary, propagate and possibly add new attribtues
\item Propagator nodes ($+, *$, log, exp, max, min, ...) --- n-ary, propagate the union of attribtues of inputs (do not add new attributes)
\item Aggregator nodes (Write, $\Sigma, \Pi, \max, \min$, ...) --- 1-ary, delete some attributes
\end{enumerate}


\section{Search Algorithm}

\subsection{Normal Form}
\begin{enumerate}
\item Figure with $\Sigma$ on top, then +, then $n$ $*$s with inputs.
\end{enumerate}

\subsection{Plan Enumeration}
\begin{enumerate}
\item Decisions
\item Memoization structure (for determining CSEs) --- E-DAG
\item Size of space (add statistics on how large the search space is in real-world ML scripts)
\item Pruning; upper and lower bounds
\end{enumerate}

\subsection{Plan Selection}
\begin{enumerate}
\item The locally-optimal solution
\item Path share and root share opportunities; path shadowing considerations
\item Algorithm ``SS-CSE'' (Search Shares of Common Sub-Expressions)
\end{enumerate}


\section{Related Work}
\label{sec:rwork}

Sum-product optimization frameworks: FAQ \cite{KhamisNR16}, AJAR, PANDA.

DB query compilation

PL comprehensions

HPC operator fusion

ML systems operator fusion

\section{Conclusions}

We have the best theory and the best system!

We have lots of future work!

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%bibliography
\small
\bibliographystyle{abbrv}
\bibliography{SpoofHolistic}  
%\enlargethispage{\baselineskip} %last row right
\normalsize


\end{document}
